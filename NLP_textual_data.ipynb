{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65a3305-ecec-40cf-8717-2b70e73539f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhoom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\bhoom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the 'punkt' tokenizer data. This is necessary for word_tokenize to work.\n",
    "# It will only download if it hasn't been downloaded before.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download 'punkt_tab' as well, as indicated by the error message.\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce616087-c47d-4960-957a-150c69bd931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"NLP is an integral part of everyday life and becoming more so as language technology is applied to diverse fields like retailing (for instance, in customer service chatbots) and medicine (interpreting or summarizing electronic health records). Conversational agents such as Amazon Alexa and Apple Siri utilize NLP to listen to user queries and find answers. The most sophisticated such agents such as GPT-3, which was recently opened for commercial applications can generate sophisticated prose on a wide variety of topics as well as power chatbots that are capable of holding coherent conversations. Google uses NLP to improve its search engine results, and social networks like Facebook use it to detect and filter hate speech. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49a424c-0f65-4461-9c43-d36b51ce1942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP is an integral part of everyday life and becoming more so as language technology is applied to diverse fields like retailing (for instance, in customer service chatbots) and medicine (interpreting or summarizing electronic health records). Conversational agents such as Amazon Alexa and Apple Siri utilize NLP to listen to user queries and find answers. The most sophisticated such agents such as GPT-3, which was recently opened for commercial applications can generate sophisticated prose on a wide variety of topics as well as power chatbots that are capable of holding coherent conversations. Google uses NLP to improve its search engine results, and social networks like Facebook use it to detect and filter hate speech. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96566f85-7ded-455b-936d-4901a0881253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8262d0a1-02cc-4af8-b746-0d8f0eb9845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b386092-fd8e-40c7-be49-afd3dbde6147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'is',\n",
       " 'an',\n",
       " 'integral',\n",
       " 'part',\n",
       " 'of',\n",
       " 'everyday',\n",
       " 'life',\n",
       " 'and',\n",
       " 'becoming',\n",
       " 'more',\n",
       " 'so',\n",
       " 'as',\n",
       " 'language',\n",
       " 'technology',\n",
       " 'is',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'diverse',\n",
       " 'fields',\n",
       " 'like',\n",
       " 'retailing',\n",
       " '(',\n",
       " 'for',\n",
       " 'instance',\n",
       " ',',\n",
       " 'in',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'chatbots',\n",
       " ')',\n",
       " 'and',\n",
       " 'medicine',\n",
       " '(',\n",
       " 'interpreting',\n",
       " 'or',\n",
       " 'summarizing',\n",
       " 'electronic',\n",
       " 'health',\n",
       " 'records',\n",
       " ')',\n",
       " '.',\n",
       " 'Conversational',\n",
       " 'agents',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Amazon',\n",
       " 'Alexa',\n",
       " 'and',\n",
       " 'Apple',\n",
       " 'Siri',\n",
       " 'utilize',\n",
       " 'NLP',\n",
       " 'to',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'user',\n",
       " 'queries',\n",
       " 'and',\n",
       " 'find',\n",
       " 'answers',\n",
       " '.',\n",
       " 'The',\n",
       " 'most',\n",
       " 'sophisticated',\n",
       " 'such',\n",
       " 'agents',\n",
       " 'such',\n",
       " 'as',\n",
       " 'GPT-3',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'recently',\n",
       " 'opened',\n",
       " 'for',\n",
       " 'commercial',\n",
       " 'applications',\n",
       " 'can',\n",
       " 'generate',\n",
       " 'sophisticated',\n",
       " 'prose',\n",
       " 'on',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'topics',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'power',\n",
       " 'chatbots',\n",
       " 'that',\n",
       " 'are',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'holding',\n",
       " 'coherent',\n",
       " 'conversations',\n",
       " '.',\n",
       " 'Google',\n",
       " 'uses',\n",
       " 'NLP',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'its',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'results',\n",
       " ',',\n",
       " 'and',\n",
       " 'social',\n",
       " 'networks',\n",
       " 'like',\n",
       " 'Facebook',\n",
       " 'use',\n",
       " 'it',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'and',\n",
       " 'filter',\n",
       " 'hate',\n",
       " 'speech',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e73376-e813-4b51-aee4-ecf556100508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhoom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#Download the 'stopwords' corpus if you haven't already\n",
    "# This will only download if it's not present\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df52684-2dca-4612-a225-6926ab2b6d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9144a8f-9b40-4bcb-9c6e-d8ecf7fa263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7198e2-9c33-42e4-85c2-1fa4f1b9305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = list(punctuation)+ stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc65357-262d-4a28-a927-699c625c1d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e932ca2-fd3f-40fc-9c28-d55352da5cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP\n",
      "integral\n",
      "part\n",
      "everyday\n",
      "life\n",
      "becoming\n",
      "language\n",
      "technology\n",
      "applied\n",
      "diverse\n",
      "fields\n",
      "like\n",
      "retailing\n",
      "instance\n",
      "customer\n",
      "service\n",
      "chatbots\n",
      "medicine\n",
      "interpreting\n",
      "summarizing\n",
      "electronic\n",
      "health\n",
      "records\n",
      "Conversational\n",
      "agents\n",
      "Amazon\n",
      "Alexa\n",
      "Apple\n",
      "Siri\n",
      "utilize\n",
      "NLP\n",
      "listen\n",
      "user\n",
      "queries\n",
      "find\n",
      "answers\n",
      "The\n",
      "sophisticated\n",
      "agents\n",
      "GPT-3\n",
      "recently\n",
      "opened\n",
      "commercial\n",
      "applications\n",
      "generate\n",
      "sophisticated\n",
      "prose\n",
      "wide\n",
      "variety\n",
      "topics\n",
      "well\n",
      "power\n",
      "chatbots\n",
      "capable\n",
      "holding\n",
      "coherent\n",
      "conversations\n",
      "Google\n",
      "uses\n",
      "NLP\n",
      "improve\n",
      "search\n",
      "engine\n",
      "results\n",
      "social\n",
      "networks\n",
      "like\n",
      "Facebook\n",
      "use\n",
      "detect\n",
      "filter\n",
      "hate\n",
      "speech\n"
     ]
    }
   ],
   "source": [
    "for i in  w:\n",
    "     if i not in stop_word:\n",
    "         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d1daaf-d839-4712-a7b3-381ee91ba07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer , RegexpStemmer, PorterStemmer , SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7290d3f-d609-4e3f-a70c-9e2f809d2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer()\n",
    "s = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856f64cb-69af-4fc5-95b3-d9f24ed414d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "connect\n",
      "connect\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "print(ls.stem(\"running\"))      # Output: run\n",
    "print(ls.stem(\"connected\"))    # Output: connect\n",
    "print(ls.stem(\"connections\"))  # Output: connect\n",
    "print(ls.stem(\"university\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be638ac-9c22-4cf2-8931-7ccae102386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn\n",
      "connect\n",
      "dog\n",
      "fix\n"
     ]
    }
   ],
   "source": [
    "rs = RegexpStemmer('ing$|ed$|s$', min=4)  # remove 'ing', 'ed', 's' if length â‰¥ 4\n",
    "print(rs.stem(\"running\"))     # Output: runn\n",
    "print(rs.stem(\"connected\"))   # Output: connect\n",
    "print(rs.stem(\"dogs\"))        # Output: dog\n",
    "print(rs.stem(\"fixing\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb879f6a-d01b-403d-978e-6db35ed9374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "connect\n",
      "connect\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"english\")\n",
    "print(ss.stem(\"running\"))      # Output: run\n",
    "print(ss.stem(\"connected\"))    # Output: connect\n",
    "print(ss.stem(\"connections\"))  # Output: connect\n",
    "print(ss.stem(\"universities\")) # Output: univers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c0d6201-1c0f-4d77-a9bf-4161b83e70a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ps.stem(\"running\"))      # Output: run\n",
    "print(ps.stem(\"connected\"))    # Output: connect\n",
    "print(ps.stem(\"connections\"))  # Output: connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c44a372f-93b1-4eea-a5d7-0b1a1d2683de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhoom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bhoom\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c3e486a-0dab-4f3b-8ea9-ad64a7ad2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl =  WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34435b2f-7cbc-4ed5-a7c5-38382dd46d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"mice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf768c66-6dee-4cab-9fd6-7a7c192bb805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Top 5 Bigrams (PMI): [('language', 'processing'), ('machine', 'learning'), ('natural', 'language'), ('love', 'machine'), ('love', 'natural')]\n",
      "ðŸ”¹ Top 5 Trigrams (PMI): [('natural', 'language', 'processing'), ('love', 'machine', 'learning'), ('love', 'natural', 'language')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhoom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample corpus\n",
    "text = \"I love natural language processing and I love machine learning\".lower()\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# -------- BIGRAM COLLLOCATION FINDING --------\n",
    "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "# Filtering: remove common words like \"and\", \"i\", etc.\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3 or w in ('and', 'i'))\n",
    "\n",
    "# Top 5 bigrams using PMI\n",
    "top_bigrams = bigram_finder.nbest(BigramAssocMeasures.pmi, 5)\n",
    "print(\"ðŸ”¹ Top 5 Bigrams (PMI):\", top_bigrams)\n",
    "\n",
    "# -------- TRIGRAM COLLLOCATION FINDING --------\n",
    "trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "trigram_finder.apply_word_filter(lambda w: len(w) < 3 or w in ('and', 'i'))\n",
    "\n",
    "# Top 5 trigrams using PMI\n",
    "top_trigrams = trigram_finder.nbest(TrigramAssocMeasures.pmi, 5)\n",
    "print(\"ðŸ”¹ Top 5 Trigrams (PMI):\", top_trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b12f57de-7c48-4ed3-9120-7e3417876808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Vocabulary: {'love': 4, 'machine': 5, 'learning': 3, 'is': 2, 'amazing': 0, 'deep': 1}\n",
      "ðŸ“Š Count Matrix:\n",
      " [[0 0 0 1 1 1]\n",
      " [1 0 1 1 0 1]\n",
      " [0 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Sample corpus\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love deep learning\"\n",
    "]\n",
    "\n",
    "#Intialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#show vocabulary (word -> column index)\n",
    "print(\"ðŸ“Œ Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Convert to array for readability\n",
    "print(\"ðŸ“Š Count Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ff5fb-33ee-4da0-b8d5-e7898440a2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
